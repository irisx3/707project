{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTvCKe1DKKNb",
        "outputId": "ca684364-822e-450e-8912-123f88f812fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  423/11597 [>.............................] - ETA: 1:01 - loss: 0.6754 - accuracy: 0.5935 - mse: 0.2405 - mae: 0.4647"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from sklearn import preprocessing\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/loan_data (1).csv')\n",
        "pd.set_option('display.max_columns', None)\n",
        "data.drop(columns=['SK_ID_CURR'], inplace=True)\n",
        "data.drop(columns=[col for col in data.columns if col.startswith('FLAG_DOCUMENT_')], inplace=True)\n",
        "data.drop(columns=[col for col in data.columns if col.startswith('EXT_SOURCE_')], inplace=True)\n",
        "\n",
        "# Fill null values depending on the column type\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column] = data[column].fillna(data[column].mode()[0])\n",
        "    elif data[column].dtype == 'float64':\n",
        "        data[column] = data[column].fillna(data[column].median())\n",
        "    elif data[column].dtype == 'int64':\n",
        "        data[column] = data[column].fillna(round(data[column].median()))\n",
        "\n",
        "# Label encoding for categorical columns\n",
        "le = preprocessing.LabelEncoder()\n",
        "for column in data.columns:\n",
        "    if data[column].dtype == 'object':\n",
        "        data[column] = le.fit_transform(data[column])\n",
        "\n",
        "# Separating the minority and majority classes\n",
        "data_target_0 = data[data.TARGET == 0]\n",
        "data_target_1 = data[data.TARGET == 1]\n",
        "\n",
        "# Upsample the minority class\n",
        "data_target_1_upsampled = resample(data_target_1, replace=True, n_samples=data_target_0.shape[0], random_state=123)\n",
        "\n",
        "# Combine the majority class with upsampled minority class\n",
        "data_upsampled = pd.concat([data_target_1_upsampled, data_target_0])\n",
        "\n",
        "# Reset the index of the upsampled dataframe\n",
        "data_upsampled.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "data_upsampled.head()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming 'data_upsampled' is already defined and preprocessed\n",
        "\n",
        "# Prepare data\n",
        "X_data = data_upsampled.drop('TARGET', axis=1)\n",
        "y_data = data_upsampled['TARGET']\n",
        "\n",
        "# Normalize your data (if not already done)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data_upsampled.drop('TARGET', axis=1))\n",
        "y = data_upsampled['TARGET']\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "# Prediction Transformer\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "\n",
        "def build_ft_transformer_model(num_features, scale=10):\n",
        "    inputs = Input(shape=(num_features,))\n",
        "    x = inputs\n",
        "    x = Reshape((1, -1))(x)  # Reshape for MultiHeadAttention\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    attn_output = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n",
        "    attn_output = Dropout(0.1)(attn_output)\n",
        "    x = LayerNormalization()(attn_output + x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'mse', 'mae'])\n",
        "    return model\n",
        "\n",
        "# Initialize model with Fourier features\n",
        "newmodel1 = build_ft_transformer_model(X_train.shape[1], scale=10)\n",
        "\n",
        "# Setup callbacks for saving the best model and early stopping\n",
        "checkpoint = ModelCheckpoint('best_model_inference.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Model training\n",
        "history = newmodel1.fit(X_train, y_train,\n",
        "                       epochs=1,\n",
        "                       batch_size=32,\n",
        "                       validation_data=(X_test, y_test),\n",
        "                       callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "\n",
        "# Load the saved model\n",
        "# model_path = '/content/best_model_inference.keras'\n",
        "# tmodel1 = load_model(model_path)\n",
        "\n",
        "import numpy as np\n",
        "# Function to generate counterfactuals\n",
        "def find_counterfactual(model, input_features, target_prediction, learning_rate=0.01, max_iterations=1000):\n",
        "    input_features = tf.Variable(input_features, dtype=tf.float32)\n",
        "    for iteration in range(max_iterations):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(input_features)\n",
        "            prediction = model(input_features[tf.newaxis, ...])\n",
        "            loss = tf.math.abs(prediction - target_prediction)\n",
        "\n",
        "        gradients = tape.gradient(loss, input_features)\n",
        "\n",
        "        if tf.reduce_all(tf.math.abs(prediction - target_prediction) < 0.01):\n",
        "            break\n",
        "\n",
        "        input_features.assign_sub(learning_rate * gradients)\n",
        "\n",
        "    return input_features.numpy()\n",
        "\n",
        "non_approved_indices = np.where(y_train.ravel() == 0)[0][:3]\n",
        "\n",
        "def plot_feature_changes(X_sample, counterfactual, title):\n",
        "    feature_changes = counterfactual - X_sample\n",
        "    top_features_indices = np.argsort(np.abs(feature_changes))[-5:]\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(5), feature_changes[top_features_indices], color='skyblue')\n",
        "    plt.xlabel('Feature Index')\n",
        "    plt.ylabel('Change in Feature Value')\n",
        "    plt.title(title)\n",
        "    plt.xticks(range(5), list(X_data.columns[list(top_features_indices)]))\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "for idx, example_index in enumerate(non_approved_indices):\n",
        "    X_sample = X_train[example_index]\n",
        "    counterfactual = find_counterfactual(newmodel1, X_sample, target_prediction=1.0)\n",
        "    plot_title = f'Top 5 Feature Changes for Training Example {example_index} Counterfactual'\n",
        "    plot_feature_changes(X_sample, counterfactual, plot_title)\n",
        "# counterfactual = find_counterfactual(newmodel1, X_sample, target_prediction=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbcN0QqoLQ46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities using the trained model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Apply the default 0.5 threshold for binary classification\n",
        "\n",
        "# Evaluation\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)  # Note: ROC-AUC is calculated from probabilities, not binary predictions\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Output the evaluation results\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
        "    X, y, data_upsampled.index, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalize your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Predicting probabilities using the trained model\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Add predictions back to the dataframe\n",
        "data_upsampled_test = data_upsampled.loc[indices_test]\n",
        "data_upsampled_test['PREDICTION'] = y_pred\n",
        "\n",
        "# Define mappings for CODE_GENDER and NAME_EDUCATION_TYPE\n",
        "gender_mapping = {0: 'F', 1: 'M', 2: 'Other'}\n",
        "education_mapping = {0: 'Academic degree', 1: 'Secondary / secondary special', 2: 'Incomplete higher', 3: 'Lower secondary', 4: 'Higher education'}\n",
        "\n",
        "# Function to calculate fairness metrics\n",
        "def calculate_fairness_metrics(data, sensitive_columns, mappings, outcome_column='TARGET', prediction_column='PREDICTION'):\n",
        "    metrics = {}\n",
        "    for sensitive_column in sensitive_columns:\n",
        "        groups = data[sensitive_column].unique()\n",
        "        group_metrics = {}\n",
        "        for group in groups:\n",
        "            group_name = mappings[sensitive_column].get(group, f'Unknown Group {group}')\n",
        "            group_data = data[data[sensitive_column] == group]\n",
        "            # Confusion matrix elements\n",
        "            tn, fp, fn, tp = confusion_matrix(group_data[outcome_column], group_data[prediction_column]).ravel()\n",
        "\n",
        "            # Calculating metrics\n",
        "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "            positive_rate = (tp + fp) / group_data.shape[0]\n",
        "\n",
        "            group_metrics[group_name] = {'TPR': tpr, 'FPR': fpr, 'Positive Rate': positive_rate}\n",
        "        metrics[sensitive_column] = group_metrics\n",
        "    return metrics\n",
        "\n",
        "# Call the function with the desired columns and mappings\n",
        "sensitive_columns = ['CODE_GENDER', 'NAME_EDUCATION_TYPE']\n",
        "mappings = {'CODE_GENDER': gender_mapping, 'NAME_EDUCATION_TYPE': education_mapping}\n",
        "fairness_metrics = calculate_fairness_metrics(data_upsampled_test, sensitive_columns, mappings)\n",
        "\n",
        "# Output the results\n",
        "for column, values in fairness_metrics.items():\n",
        "    print(f\"Fairness metrics for {column}:\")\n",
        "    for group_name, metrics in values.items():\n",
        "        print(f\"  {group_name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"    {metric}: {value:.2f}\")\n",
        "\n",
        "# Function to calculate additional fairness metrics\n",
        "def calculate_additional_fairness_metrics(data, sensitive_columns, mappings, outcome_column='TARGET', prediction_column='PREDICTION'):\n",
        "    metrics = {}\n",
        "    for sensitive_column in sensitive_columns:\n",
        "        groups = data[sensitive_column].unique()\n",
        "        group_metrics = {}\n",
        "        for group in groups:\n",
        "            group_name = mappings[sensitive_column].get(group, f'Unknown Group {group}')\n",
        "            group_data = data[data[sensitive_column] == group]\n",
        "            # Accuracy\n",
        "            accuracy = accuracy_score(group_data[outcome_column], group_data[prediction_column])\n",
        "            # Confusion matrix elements\n",
        "            tn, fp, fn, tp = confusion_matrix(group_data[outcome_column], group_data[prediction_column]).ravel()\n",
        "\n",
        "            # Calculating metrics\n",
        "            total = tn + fp + fn + tp\n",
        "            pr = (tp + fp) / total if total > 0 else 0\n",
        "            tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "            disparity = pr / max(tpr, 1e-8)  # to avoid division by zero\n",
        "\n",
        "            # Theil index\n",
        "            p = pr\n",
        "            q = 1 - pr\n",
        "            t_index = 0 if p == 0 else p * np.log(p / q)\n",
        "\n",
        "            group_metrics[group_name] = {'Accuracy': accuracy, 'Disparate Impact': disparity, 'Theil Index': t_index}\n",
        "        metrics[sensitive_column] = group_metrics\n",
        "    return metrics\n",
        "\n",
        "# Call the function with the desired columns and mappings\n",
        "sensitive_columns = ['CODE_GENDER', 'NAME_EDUCATION_TYPE']\n",
        "mappings = {'CODE_GENDER': gender_mapping, 'NAME_EDUCATION_TYPE': education_mapping}\n",
        "additional_fairness_metrics = calculate_additional_fairness_metrics(data_upsampled_test, sensitive_columns, mappings)\n",
        "\n",
        "# Output the results\n",
        "for column, values in additional_fairness_metrics.items():\n",
        "    print(f\"Additional fairness metrics for {column}:\")\n",
        "    for group_name, metrics in values.items():\n",
        "        print(f\"  {group_name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_curve, f1_score, roc_auc_score, confusion_matrix, mean_squared_error, mean_absolute_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten, Reshape\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Load and preprocess data\n",
        "\n",
        "X = data_upsampled.drop('TARGET', axis=1)\n",
        "y = data_upsampled['TARGET']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data before applying SMOTE to avoid data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Apply SMOTE only to training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "            print(f\"    {metric}: {value:.2f}\")"
      ],
      "metadata": {
        "id": "8JCaSqQFKQ6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FourierFeatures(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_features, scale=10, **kwargs):\n",
        "        super(FourierFeatures, self).__init__(**kwargs)\n",
        "        self.num_features = num_features\n",
        "        self.scale = scale\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs\n",
        "        for _ in range(self.num_features):\n",
        "            x = tf.concat([x, tf.sin(x * self.scale), tf.cos(x * self.scale)], axis=-1)\n",
        "        return x\n",
        "\n",
        "def build_ft_transformer_model(num_features, fourier_features=True, scale=10):\n",
        "    inputs = Input(shape=(num_features,))\n",
        "    x = inputs\n",
        "    if fourier_features:\n",
        "        x = Dense(64, activation='relu')(inputs)\n",
        "        x = LayerNormalization()(x)\n",
        "        # Apply Fourier transformations directly in the Dense layer\n",
        "        x = tf.concat([x, tf.sin(x * scale), tf.cos(x * scale)], axis=-1)\n",
        "    x = Reshape((1, -1))(x)  # Reshape for MultiHeadAttention\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    attn_output = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n",
        "    attn_output = Dropout(0.1)(attn_output)\n",
        "    x = LayerNormalization()(attn_output + x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy', 'mse', 'mae'])\n",
        "    return model\n",
        "\n",
        "# Initialize model with Fourier features\n",
        "newmodel1 = build_ft_transformer_model(X_train_smote.shape[1], fourier_features=True, scale=10)\n",
        "\n",
        "# Setup callbacks for saving the best model and early stopping\n",
        "checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Model training\n",
        "history = newmodel1.fit(X_train_smote, y_train_smote,\n",
        "                       epochs=50,\n",
        "                       batch_size=32,\n",
        "                       validation_data=(X_test, y_test),\n",
        "                       callbacks=[checkpoint, early_stopping])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "plt.figure(figsize=(14, 4))\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Test Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation MSE values\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['mse'], label='Train MSE')\n",
        "plt.plot(history.history['val_mse'], label='Test MSE')\n",
        "plt.title('Model MSE')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation MAE values\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['mae'], label='Train MAE')\n",
        "plt.plot(history.history['val_mae'], label='Test MAE')\n",
        "plt.title('Model MAE')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1kcvLZnZLxAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "def adjust_thresholds(y_true, y_pred_prob):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "    # Maximize Youden's index (J = TPR - FPR)\n",
        "    j_index = tpr - fpr\n",
        "    optimal_idx = np.argmax(j_index)\n",
        "    optimal_threshold = thresholds[optimal_idx]\n",
        "    return optimal_threshold\n",
        "\n",
        "# Predictions and threshold adjustment\n",
        "y_pred_prob = newmodel1.predict(X_test)\n",
        "optimal_threshold = adjust_thresholds(y_test, y_pred_prob)  # Ensure adjust_thresholds is correctly defined\n",
        "y_pred_adjusted = (y_pred_prob > optimal_threshold).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "f1 = f1_score(y_test, y_pred_adjusted)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "cm = confusion_matrix(y_test, y_pred_adjusted)\n",
        "mse = mean_squared_error(y_test, y_pred_adjusted)\n",
        "mae = mean_absolute_error(y_test, y_pred_adjusted)\n",
        "\n",
        "print(f\"F1 Score: {f1:.2f}\")\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
        "    X, y, data_upsampled.index, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalize your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Predicting probabilities using the trained model\n",
        "y_pred_prob = newmodel1.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Add predictions back to the dataframe\n",
        "data_upsampled_test = data_upsampled.loc[indices_test]\n",
        "data_upsampled_test['PREDICTION'] = y_pred\n",
        "fairness_metrics = calculate_fairness_metrics(data_upsampled_test, sensitive_columns, mappings)\n",
        "\n",
        "# Output the results\n",
        "for column, values in fairness_metrics.items():\n",
        "    print(f\"Fairness metrics for {column}:\")\n",
        "    for group_name, metrics in values.items():\n",
        "        print(f\"  {group_name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"    {metric}: {value:.2f}\")\n",
        "\n",
        "additional_fairness_metrics = calculate_additional_fairness_metrics(data_upsampled_test, sensitive_columns, mappings)\n",
        "\n",
        "# Output the results\n",
        "for column, values in additional_fairness_metrics.items():\n",
        "    print(f\"Additional fairness metrics for {column}:\")\n",
        "    for group_name, metrics in values.items():\n",
        "        print(f\"  {group_name}:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"    {metric}: {value:.2f}\")"
      ],
      "metadata": {
        "id": "PnI0KvYzMSwW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}