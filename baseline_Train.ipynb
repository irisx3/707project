{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0vEcPxSJ8hI",
        "outputId": "fdb755ba-d9d2-42ed-b293-87b110fcae36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
            "  Cloning https://github.com/AI4Finance-Foundation/FinRL.git to /tmp/pip-req-build-tt4d6vhu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/FinRL.git /tmp/pip-req-build-tt4d6vhu\n",
            "  Resolved https://github.com/AI4Finance-Foundation/FinRL.git to commit d25d902a6de54931a329adc38a2663e8f576adc4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.8)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-iuha8jjc/elegantrl_8a27d7566d1948338f8c32350ec29b0a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-iuha8jjc/elegantrl_8a27d7566d1948338f8c32350ec29b0a\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 5e828af1503098f4da046c0f12432dbd4ef8bd97\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-py<0.38,>=0.37 (from finrl==0.3.8)\n",
            "  Downloading alpaca_py-0.37.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jqdatasdk<2,>=1 (from finrl==0.3.8)\n",
            "  Downloading jqdatasdk-1.9.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pyfolio-reloaded<0.10,>=0.9 (from finrl==0.3.8)\n",
            "  Downloading pyfolio_reloaded-0.9.8-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.8)\n",
            "  Downloading pyportfolioopt-1.5.6-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ray<3,>=2 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (1.6.1)\n",
            "Collecting selenium<5,>=4 (from finrl==0.3.8)\n",
            "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting stable-baselines3>=2.0.0a5 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading stable_baselines3-2.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting stockstats<0.6,>=0.5 (from finrl==0.3.8)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl.metadata (26 kB)\n",
            "Collecting webdriver-manager<5,>=4 (from finrl==0.3.8)\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.8)\n",
            "  Downloading wrds-3.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.8) (0.2.55)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (1.1.0)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.32.3)\n",
            "Collecting sseclient-py<2.0.0,>=1.7.2 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from alpaca-py<0.38,>=0.37->finrl==0.3.8) (15.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (2.0.2)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (1.8.0)\n",
            "Collecting websockets>=10.4 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack<2.0.0,>=1.0.3 (from alpaca-py<0.38,>=0.37->finrl==0.3.8)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.8) (3.11.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.8)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.8) (24.2)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (75.2.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (2025.1.31)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (43.0.3)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.8)\n",
            "  Downloading aiodns-3.2.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.8) (1.19.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (1.17.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.8) (2.0.40)\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.8)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting thriftpy2!=0.5.1,>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.8)\n",
            "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.14.1)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.13.2)\n",
            "Collecting empyrical-reloaded>=0.5.9 (from pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8)\n",
            "  Downloading empyrical_reloaded-0.5.11-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cvxpy>=1.1.19 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (1.6.4)\n",
            "Collecting ecos<3.0.0,>=2.0.14 (from pyportfolioopt<2,>=1->finrl==0.3.8)\n",
            "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.8) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (4.23.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (5.29.4)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (1.5.0)\n",
            "Collecting aiohttp_cors (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (1.71.0)\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (18.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.8) (2025.3.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.8) (3.6.0)\n",
            "Collecting trio~=0.17 (from selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium<5,>=4->finrl==0.3.8) (4.13.1)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (11.1.0)\n",
            "Collecting python-dotenv (from webdriver-manager<5,>=4->finrl==0.3.8)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.8)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.8) (4.13.3)\n",
            "Collecting th (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8)\n",
            "  Downloading th-0.4.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.8)\n",
            "  Downloading pycares-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (2.6.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (25.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.8) (0.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.8) (2.6)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (1.17.1)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (1.0.3)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (0.10.0)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.11/dist-packages (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.8) (3.2.7.post2)\n",
            "Requirement already satisfied: bottleneck>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from empyrical-reloaded>=0.5.9->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.2)\n",
            "Collecting peewee>=3.16.2 (from yfinance<0.3,>=0.2->finrl==0.3.8)\n",
            "  Downloading peewee-3.17.3.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.8) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py<0.38,>=0.37->finrl==0.3.8) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.30.0->alpaca-py<0.38,>=0.37->finrl==0.3.8) (3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.8) (3.1.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.3)\n",
            "Requirement already satisfied: Cython>=3.0.10 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.0.12)\n",
            "Requirement already satisfied: ply<4.0,>=3.4 in /usr/local/lib/python3.11/dist-packages (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.8) (3.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (1.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (2.4.0)\n",
            "Collecting outcome (from trio~=0.17->selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium<5,>=4->finrl==0.3.8) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium<5,>=4->finrl==0.3.8) (1.7.1)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.8) (0.24.0)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.24.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.8) (1.17.2)\n",
            "Collecting niltype<2.0,>=0.3 (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.8)\n",
            "  Downloading niltype-1.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.8) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.69.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=3.2.3->pyfolio-reloaded<0.10,>=0.9->finrl==0.3.8) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.8) (3.0.2)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium<5,>=4->finrl==0.3.8) (0.14.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.8) (0.6.1)\n",
            "Downloading alpaca_py-0.37.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyfolio_reloaded-0.9.8-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyportfolioopt-1.5.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl (68.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.6.0-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.5/184.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading wrds-3.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
            "Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading empyrical_reloaded-0.5.11-py3-none-any.whl (32 kB)\n",
            "Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading th-0.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading niltype-1.0.2-py3-none-any.whl (5.3 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading pycares-4.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.3/289.3 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: finrl, msgpack, elegantrl, peewee, thriftpy2\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.8-py3-none-any.whl size=4702655 sha256=29575c4030eb7037667ac4d976ce285a35c463e68b3d0652ebe868f63d965dd8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ou7qxb1g/wheels/22/04/d2/8ee1f0ed6a91622a6548d244772ae124a9b3795372817286a0\n",
            "  Building wheel for msgpack (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp311-cp311-linux_x86_64.whl size=15688 sha256=2cd562cb943d21a4297f10d6de057a78e5553affdd676c4b83ff8856eaf8e9c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/35/da/ed9b26b510235e00e3a3c3bab7bad97b59214729662255ab3d\n",
            "  Building wheel for elegantrl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=ElegantRL-0.3.10-py3-none-any.whl size=272005 sha256=22e5a38ca5bbc39befdf786c60c6166ca21eb17f69004685863ca61ef060bfb8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ou7qxb1g/wheels/9a/77/4d/6284111037b2dd64af9ef18d4d600d9c185cc2f6f09704e896\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peewee: filename=peewee-3.17.3-cp311-cp311-linux_x86_64.whl size=763054 sha256=ddcd1f48f76c00e8024b637b2329216c6f7ee009dbdeb60994c2e7c248494d4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/3d/30/a54ad8c2307aa653f234a6f651ab12fbc5cfbb3f383145ab6a\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp311-cp311-linux_x86_64.whl size=1824831 sha256=e64ecb2e3393f10809c28fd44ba2952f4500dc9cedf4094ff13320b63b9b4ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/32/fa/51ae0364792430fb80858f4705c9e0cba3d6900f591f0c4495\n",
            "Successfully built finrl msgpack elegantrl peewee thriftpy2\n",
            "Installing collected packages: sseclient-py, py-spy, peewee, opencensus-context, msgpack, distlib, colorful, wsproto, websockets, virtualenv, urllib3, thriftpy2, tensorboardX, PyYAML, python-dotenv, pymysql, psycopg2-binary, outcome, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, niltype, jedi, deprecation, trio, th, pycares, nvidia-cusparse-cu12, nvidia-cudnn-cu12, ecos, wrds, webdriver-manager, trio-websocket, stockstats, nvidia-cusolver-cu12, jqdatasdk, empyrical-reloaded, elegantrl, alpaca-trade-api, alpaca-py, aiohttp_cors, aiodns, selenium, ray, pyportfolioopt, pyfolio-reloaded, opencensus, ccxt, stable-baselines3, finrl\n",
            "  Attempting uninstall: peewee\n",
            "    Found existing installation: peewee 3.17.9\n",
            "    Uninstalling peewee-3.17.9:\n",
            "      Successfully uninstalled peewee-3.17.9\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.10.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyYAML-6.0.1 aiodns-3.2.0 aiohttp_cors-0.8.1 alpaca-py-0.37.0 alpaca-trade-api-3.2.0 ccxt-3.1.60 colorful-0.5.6 deprecation-2.1.0 distlib-0.3.9 ecos-2.0.14 elegantrl-0.3.10 empyrical-reloaded-0.5.11 finrl-0.3.8 jedi-0.19.2 jqdatasdk-1.9.7 msgpack-1.0.3 niltype-1.0.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opencensus-0.11.4 opencensus-context-0.1.3 outcome-1.3.0.post0 peewee-3.17.3 psycopg2-binary-2.9.10 py-spy-0.4.0 pycares-4.6.0 pyfolio-reloaded-0.9.8 pymysql-1.1.1 pyportfolioopt-1.5.6 python-dotenv-1.1.0 ray-2.44.1 selenium-4.31.0 sseclient-py-1.8.0 stable-baselines3-2.6.0 stockstats-0.5.4 tensorboardX-2.6.2.2 th-0.4.1 thriftpy2-0.5.2 trio-0.29.0 trio-websocket-0.12.2 urllib3-1.26.20 virtualenv-20.30.0 webdriver-manager-4.0.2 websockets-10.4 wrds-3.3.0 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "#part of the codes are from https://github.com/AI4Finance-Foundation/FinRL\n",
        "## install finrl library\n",
        "!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xt1317y2ixSS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from stable_baselines3.common.logger import configure\n",
        "\n",
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
        "from finrl.main import check_and_make_directories\n",
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "\n",
        "check_and_make_directories([TRAINED_MODEL_DIR])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwohnPki1HBq",
        "outputId": "dac33bfd-c234-4f22-835d-aa043d650bd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandas_market_calendars\n",
            "  Downloading pandas_market_calendars-5.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: pandas>=1.1 in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from pandas_market_calendars) (2.8.2)\n",
            "Collecting exchange-calendars>=3.3 (from pandas_market_calendars)\n",
            "  Downloading exchange_calendars-4.10-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (2.0.2)\n",
            "Collecting pyluach (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange-calendars>=3.3->pandas_market_calendars) (0.12.1)\n",
            "Collecting korean_lunar_calendar (from exchange-calendars>=3.3->pandas_market_calendars)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1->pandas_market_calendars) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->pandas_market_calendars) (1.17.0)\n",
            "Downloading pandas_market_calendars-5.0.0-py3-none-any.whl (122 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.0/123.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.10-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.4/198.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: korean_lunar_calendar, pyluach, exchange-calendars, pandas_market_calendars\n",
            "Successfully installed exchange-calendars-4.10 korean_lunar_calendar-0.3.1 pandas_market_calendars-5.0.0 pyluach-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas_market_calendars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3H88JXkI93v"
      },
      "source": [
        "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
        "\n",
        "state-action-reward are specified as follows:\n",
        "\n",
        "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
        "\n",
        "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
        "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
        "\n",
        "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
        "\n",
        "\n",
        "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFCP1YEhi6oi"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('train_data.csv')\n",
        "train = train.set_index(train.columns[0])\n",
        "train.index.names = ['']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ko7ZjrxvJny",
        "outputId": "33ad9bc2-cc04-4dd7-9aad-36b2ef7cb54c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['.config', 'drive', 'trained_models', 'sample_data']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.listdir())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Saamv9ZIqxC1",
        "outputId": "6f137802-a027-46e9-f442-d759bc9a6a75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T3DZPoaIm8k",
        "outputId": "8567a02f-ae14-41df-a9cf-411bccab4dbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stock Dimension: 29, State Space: 291\n"
          ]
        }
      ],
      "source": [
        "#construct environment\n",
        "stock_dimension = len(train.tic.unique())\n",
        "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WsOLoeNcJF8Q"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}\n",
        "\n",
        "\n",
        "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aS-SHiGRJK-4",
        "outputId": "d76c5f8d-ad3f-4f26-b388-7957a3403aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
          ]
        }
      ],
      "source": [
        "env_train, _ = e_train_gym.get_sb_env()\n",
        "print(type(env_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "364PsqckttcQ"
      },
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "\n",
        "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
        "if_using_a2c = True\n",
        "if_using_ddpg = True\n",
        "if_using_ppo = True\n",
        "if_using_td3 = True\n",
        "if_using_sac = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDmqOyF9h1iz"
      },
      "source": [
        "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uijiWgkuh1jB"
      },
      "source": [
        "### Agent 1: A2C\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "607366ff-d0a8-4534-c0e4-600b71389eda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
            "Using cuda device\n",
            "Logging to results/a2c\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run A2C on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_a2c = agent.get_model(\"a2c\")\n",
        "\n",
        "if if_using_a2c:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/a2c'\n",
        "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_a2c.set_logger(new_logger_a2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVpkWGqH4-D",
        "outputId": "59bfd772-8458-433d-f236-fe34f5e29d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 66           |\n",
            "|    iterations         | 100          |\n",
            "|    time_elapsed       | 7            |\n",
            "|    total_timesteps    | 500          |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.3        |\n",
            "|    explained_variance | -0.215       |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 99           |\n",
            "|    policy_loss        | -3.86        |\n",
            "|    reward             | 0.0044633783 |\n",
            "|    std                | 1            |\n",
            "|    value_loss         | 0.104        |\n",
            "----------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 81         |\n",
            "|    iterations         | 200        |\n",
            "|    time_elapsed       | 12         |\n",
            "|    total_timesteps    | 1000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.00418   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 199        |\n",
            "|    policy_loss        | -88.5      |\n",
            "|    reward             | -1.0633011 |\n",
            "|    std                | 1          |\n",
            "|    value_loss         | 5.18       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 80        |\n",
            "|    iterations         | 300       |\n",
            "|    time_elapsed       | 18        |\n",
            "|    total_timesteps    | 1500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.3     |\n",
            "|    explained_variance | -0.0242   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 299       |\n",
            "|    policy_loss        | -200      |\n",
            "|    reward             | 6.2951946 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 24.9      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 85       |\n",
            "|    iterations         | 400      |\n",
            "|    time_elapsed       | 23       |\n",
            "|    total_timesteps    | 2000     |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.3    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 399      |\n",
            "|    policy_loss        | 74       |\n",
            "|    reward             | 1.193747 |\n",
            "|    std                | 1.01     |\n",
            "|    value_loss         | 11.4     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 88         |\n",
            "|    iterations         | 500        |\n",
            "|    time_elapsed       | 28         |\n",
            "|    total_timesteps    | 2500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 499        |\n",
            "|    policy_loss        | 1.27e+03   |\n",
            "|    reward             | -17.258219 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 900        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 87          |\n",
            "|    iterations         | 600         |\n",
            "|    time_elapsed       | 34          |\n",
            "|    total_timesteps    | 3000        |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.3       |\n",
            "|    explained_variance | 0.675       |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 599         |\n",
            "|    policy_loss        | 174         |\n",
            "|    reward             | -0.22723997 |\n",
            "|    std                | 1           |\n",
            "|    value_loss         | 17.2        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 700        |\n",
            "|    time_elapsed       | 39         |\n",
            "|    total_timesteps    | 3500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | -0.662     |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 699        |\n",
            "|    policy_loss        | -117       |\n",
            "|    reward             | -4.0898185 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 9.92       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 87         |\n",
            "|    iterations         | 800        |\n",
            "|    time_elapsed       | 45         |\n",
            "|    total_timesteps    | 4000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.3      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 799        |\n",
            "|    policy_loss        | 12.5       |\n",
            "|    reward             | -2.0919313 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 1.06       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 89         |\n",
            "|    iterations         | 900        |\n",
            "|    time_elapsed       | 50         |\n",
            "|    total_timesteps    | 4500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -0.00256   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 899        |\n",
            "|    policy_loss        | 262        |\n",
            "|    reward             | -1.3968328 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 46         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 1000       |\n",
            "|    time_elapsed       | 55         |\n",
            "|    total_timesteps    | 5000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 999        |\n",
            "|    policy_loss        | 28.6       |\n",
            "|    reward             | -6.6657667 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 4.13       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 89        |\n",
            "|    iterations         | 1100      |\n",
            "|    time_elapsed       | 61        |\n",
            "|    total_timesteps    | 5500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.4     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1099      |\n",
            "|    policy_loss        | -544      |\n",
            "|    reward             | 4.2717676 |\n",
            "|    std                | 1.01      |\n",
            "|    value_loss         | 193       |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 1200       |\n",
            "|    time_elapsed       | 66         |\n",
            "|    total_timesteps    | 6000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.4      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1199       |\n",
            "|    policy_loss        | -118       |\n",
            "|    reward             | 0.47966975 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 10.1       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 88         |\n",
            "|    iterations         | 1300       |\n",
            "|    time_elapsed       | 73         |\n",
            "|    total_timesteps    | 6500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.5      |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1299       |\n",
            "|    policy_loss        | 41.9       |\n",
            "|    reward             | -4.7214217 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 7.43       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 89        |\n",
            "|    iterations         | 1400      |\n",
            "|    time_elapsed       | 77        |\n",
            "|    total_timesteps    | 7000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1399      |\n",
            "|    policy_loss        | 167       |\n",
            "|    reward             | 1.1671104 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 23.6      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 1500       |\n",
            "|    time_elapsed       | 83         |\n",
            "|    total_timesteps    | 7500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1499       |\n",
            "|    policy_loss        | 417        |\n",
            "|    reward             | 0.33658087 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 126        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 90        |\n",
            "|    iterations         | 1600      |\n",
            "|    time_elapsed       | 88        |\n",
            "|    total_timesteps    | 8000      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1599      |\n",
            "|    policy_loss        | -411      |\n",
            "|    reward             | 1.6240264 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 109       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 90        |\n",
            "|    iterations         | 1700      |\n",
            "|    time_elapsed       | 93        |\n",
            "|    total_timesteps    | 8500      |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 1699      |\n",
            "|    policy_loss        | -241      |\n",
            "|    reward             | 3.2930095 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 60        |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 1800       |\n",
            "|    time_elapsed       | 99         |\n",
            "|    total_timesteps    | 9000       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 1.19e-07   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1799       |\n",
            "|    policy_loss        | -36.5      |\n",
            "|    reward             | 0.28846323 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 0.93       |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 1900       |\n",
            "|    time_elapsed       | 104        |\n",
            "|    total_timesteps    | 9500       |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1899       |\n",
            "|    policy_loss        | 41.5       |\n",
            "|    reward             | 0.14978601 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 2.5        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 2000       |\n",
            "|    time_elapsed       | 110        |\n",
            "|    total_timesteps    | 10000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.6      |\n",
            "|    explained_variance | -0.00102   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 1999       |\n",
            "|    policy_loss        | -136       |\n",
            "|    reward             | 0.51232725 |\n",
            "|    std                | 1.01       |\n",
            "|    value_loss         | 21.1       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 90        |\n",
            "|    iterations         | 2100      |\n",
            "|    time_elapsed       | 115       |\n",
            "|    total_timesteps    | 10500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2099      |\n",
            "|    policy_loss        | 191       |\n",
            "|    reward             | 2.4509244 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 36.7      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 2200       |\n",
            "|    time_elapsed       | 120        |\n",
            "|    total_timesteps    | 11000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2199       |\n",
            "|    policy_loss        | -217       |\n",
            "|    reward             | -2.0314267 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 42         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 90         |\n",
            "|    iterations         | 2300       |\n",
            "|    time_elapsed       | 127        |\n",
            "|    total_timesteps    | 11500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2299       |\n",
            "|    policy_loss        | -2.29e+03  |\n",
            "|    reward             | -17.610233 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 3.25e+03   |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 2400       |\n",
            "|    time_elapsed       | 131        |\n",
            "|    total_timesteps    | 12000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 2399       |\n",
            "|    policy_loss        | 79         |\n",
            "|    reward             | 0.62076116 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 10.3       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 90          |\n",
            "|    iterations         | 2500        |\n",
            "|    time_elapsed       | 137         |\n",
            "|    total_timesteps    | 12500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.7       |\n",
            "|    explained_variance | 1.79e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 2499        |\n",
            "|    policy_loss        | -24.9       |\n",
            "|    reward             | 0.059615348 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 0.794       |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 90       |\n",
            "|    iterations         | 2600     |\n",
            "|    time_elapsed       | 142      |\n",
            "|    total_timesteps    | 13000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 1.19e-07 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2599     |\n",
            "|    policy_loss        | 73.7     |\n",
            "|    reward             | 3.873065 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 5.05     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 2700      |\n",
            "|    time_elapsed       | 147       |\n",
            "|    total_timesteps    | 13500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2699      |\n",
            "|    policy_loss        | 13.9      |\n",
            "|    reward             | -1.827723 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 1.05      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 90       |\n",
            "|    iterations         | 2800     |\n",
            "|    time_elapsed       | 153      |\n",
            "|    total_timesteps    | 14000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.6    |\n",
            "|    explained_variance | 0.0173   |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2799     |\n",
            "|    policy_loss        | 252      |\n",
            "|    reward             | 8.526938 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 56.4     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 2900      |\n",
            "|    time_elapsed       | 158       |\n",
            "|    total_timesteps    | 14500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.6     |\n",
            "|    explained_variance | -2.38e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 2899      |\n",
            "|    policy_loss        | -116      |\n",
            "|    reward             | 2.1153502 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 8.33      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 90       |\n",
            "|    iterations         | 3000     |\n",
            "|    time_elapsed       | 165      |\n",
            "|    total_timesteps    | 15000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.7    |\n",
            "|    explained_variance | 0.00701  |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 2999     |\n",
            "|    policy_loss        | -15.1    |\n",
            "|    reward             | 1.089018 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 1.08     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3100      |\n",
            "|    time_elapsed       | 169       |\n",
            "|    total_timesteps    | 15500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3099      |\n",
            "|    policy_loss        | 167       |\n",
            "|    reward             | 0.5890128 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 25.8      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 3200       |\n",
            "|    time_elapsed       | 174        |\n",
            "|    total_timesteps    | 16000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3199       |\n",
            "|    policy_loss        | -266       |\n",
            "|    reward             | -4.0746317 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 88         |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 3300       |\n",
            "|    time_elapsed       | 180        |\n",
            "|    total_timesteps    | 16500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.7      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 3299       |\n",
            "|    policy_loss        | 338        |\n",
            "|    reward             | -7.2030416 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 67.3       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3400      |\n",
            "|    time_elapsed       | 185       |\n",
            "|    total_timesteps    | 17000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3399      |\n",
            "|    policy_loss        | 816       |\n",
            "|    reward             | 12.386593 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 456       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3500      |\n",
            "|    time_elapsed       | 191       |\n",
            "|    total_timesteps    | 17500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3499      |\n",
            "|    policy_loss        | 69.4      |\n",
            "|    reward             | 1.2000309 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 4.34      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3600      |\n",
            "|    time_elapsed       | 196       |\n",
            "|    total_timesteps    | 18000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.7     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3599      |\n",
            "|    policy_loss        | -26.1     |\n",
            "|    reward             | 1.5816369 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 2.67      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 91       |\n",
            "|    iterations         | 3700     |\n",
            "|    time_elapsed       | 201      |\n",
            "|    total_timesteps    | 18500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.8    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 3699     |\n",
            "|    policy_loss        | 170      |\n",
            "|    reward             | 4.143682 |\n",
            "|    std                | 1.02     |\n",
            "|    value_loss         | 20.5     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3800      |\n",
            "|    time_elapsed       | 207       |\n",
            "|    total_timesteps    | 19000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3799      |\n",
            "|    policy_loss        | 164       |\n",
            "|    reward             | 1.5599993 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 19.8      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 3900      |\n",
            "|    time_elapsed       | 212       |\n",
            "|    total_timesteps    | 19500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3899      |\n",
            "|    policy_loss        | -216      |\n",
            "|    reward             | 0.6607369 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 26.9      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 4000      |\n",
            "|    time_elapsed       | 218       |\n",
            "|    total_timesteps    | 20000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0.00584   |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 3999      |\n",
            "|    policy_loss        | -293      |\n",
            "|    reward             | 3.7999704 |\n",
            "|    std                | 1.02      |\n",
            "|    value_loss         | 48.2      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 91          |\n",
            "|    iterations         | 4100        |\n",
            "|    time_elapsed       | 223         |\n",
            "|    total_timesteps    | 20500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | 0.0832      |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4099        |\n",
            "|    policy_loss        | -28.9       |\n",
            "|    reward             | -0.13552386 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 1.19        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 91          |\n",
            "|    iterations         | 4200        |\n",
            "|    time_elapsed       | 228         |\n",
            "|    total_timesteps    | 21000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4199        |\n",
            "|    policy_loss        | -119        |\n",
            "|    reward             | -0.43236268 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 10.3        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 91       |\n",
            "|    iterations         | 4300     |\n",
            "|    time_elapsed       | 234      |\n",
            "|    total_timesteps    | 21500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4299     |\n",
            "|    policy_loss        | -146     |\n",
            "|    reward             | 5.156451 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 13.2     |\n",
            "------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 4400      |\n",
            "|    time_elapsed       | 239       |\n",
            "|    total_timesteps    | 22000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4399      |\n",
            "|    policy_loss        | 144       |\n",
            "|    reward             | 3.9427114 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 21        |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 91          |\n",
            "|    iterations         | 4500        |\n",
            "|    time_elapsed       | 245         |\n",
            "|    total_timesteps    | 22500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.9       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4499        |\n",
            "|    policy_loss        | 656         |\n",
            "|    reward             | -0.46357918 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 317         |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 4600      |\n",
            "|    time_elapsed       | 250       |\n",
            "|    total_timesteps    | 23000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 4599      |\n",
            "|    policy_loss        | 94.5      |\n",
            "|    reward             | 4.5365624 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 9.34      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 4700       |\n",
            "|    time_elapsed       | 255        |\n",
            "|    total_timesteps    | 23500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4699       |\n",
            "|    policy_loss        | -119       |\n",
            "|    reward             | 0.66764504 |\n",
            "|    std                | 1.02       |\n",
            "|    value_loss         | 11.2       |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 91          |\n",
            "|    iterations         | 4800        |\n",
            "|    time_elapsed       | 261         |\n",
            "|    total_timesteps    | 24000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -41.8       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 4799        |\n",
            "|    policy_loss        | -67.9       |\n",
            "|    reward             | -0.16906615 |\n",
            "|    std                | 1.02        |\n",
            "|    value_loss         | 2.92        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 91       |\n",
            "|    iterations         | 4900     |\n",
            "|    time_elapsed       | 266      |\n",
            "|    total_timesteps    | 24500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 4899     |\n",
            "|    policy_loss        | 71.2     |\n",
            "|    reward             | 1.481637 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 4.08     |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 5000       |\n",
            "|    time_elapsed       | 272        |\n",
            "|    total_timesteps    | 25000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.8      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 4999       |\n",
            "|    policy_loss        | -88.1      |\n",
            "|    reward             | -1.1002771 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 6.39       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 5100      |\n",
            "|    time_elapsed       | 277       |\n",
            "|    total_timesteps    | 25500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.8     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5099      |\n",
            "|    policy_loss        | -228      |\n",
            "|    reward             | 3.5103376 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 54.3      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 91       |\n",
            "|    iterations         | 5200     |\n",
            "|    time_elapsed       | 282      |\n",
            "|    total_timesteps    | 26000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5199     |\n",
            "|    policy_loss        | -529     |\n",
            "|    reward             | 9.97169  |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 184      |\n",
            "------------------------------------\n",
            "day: 2892, episode: 10\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6384437.09\n",
            "total_reward: 5384437.09\n",
            "total_cost: 11325.23\n",
            "total_trades: 47443\n",
            "Sharpe: 1.014\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 91           |\n",
            "|    iterations         | 5300         |\n",
            "|    time_elapsed       | 288          |\n",
            "|    total_timesteps    | 26500        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -41.9        |\n",
            "|    explained_variance | 0            |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 5299         |\n",
            "|    policy_loss        | -96.6        |\n",
            "|    reward             | -0.035466693 |\n",
            "|    std                | 1.03         |\n",
            "|    value_loss         | 5.38         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 5400      |\n",
            "|    time_elapsed       | 293       |\n",
            "|    total_timesteps    | 27000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5399      |\n",
            "|    policy_loss        | -95.8     |\n",
            "|    reward             | 0.2777536 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 5.73      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 5500      |\n",
            "|    time_elapsed       | 299       |\n",
            "|    total_timesteps    | 27500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | 8.37e-05  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5499      |\n",
            "|    policy_loss        | 285       |\n",
            "|    reward             | 3.3181255 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 57.1      |\n",
            "-------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 91       |\n",
            "|    iterations         | 5600     |\n",
            "|    time_elapsed       | 304      |\n",
            "|    total_timesteps    | 28000    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -41.9    |\n",
            "|    explained_variance | 0        |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 5599     |\n",
            "|    policy_loss        | -146     |\n",
            "|    reward             | 1.887971 |\n",
            "|    std                | 1.03     |\n",
            "|    value_loss         | 20       |\n",
            "------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 5700       |\n",
            "|    time_elapsed       | 310        |\n",
            "|    total_timesteps    | 28500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -41.9      |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5699       |\n",
            "|    policy_loss        | -15.1      |\n",
            "|    reward             | 0.74807304 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 3.17       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 5800      |\n",
            "|    time_elapsed       | 315       |\n",
            "|    total_timesteps    | 29000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -41.9     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 5799      |\n",
            "|    policy_loss        | -17.6     |\n",
            "|    reward             | 1.7266252 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.58      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 5900        |\n",
            "|    time_elapsed       | 320         |\n",
            "|    total_timesteps    | 29500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 5899        |\n",
            "|    policy_loss        | 70.9        |\n",
            "|    reward             | -0.98417324 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 6.22        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 6000       |\n",
            "|    time_elapsed       | 326        |\n",
            "|    total_timesteps    | 30000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 5999       |\n",
            "|    policy_loss        | 95.5       |\n",
            "|    reward             | 0.17316781 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 7.02       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 6100      |\n",
            "|    time_elapsed       | 331       |\n",
            "|    total_timesteps    | 30500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6099      |\n",
            "|    policy_loss        | -90.3     |\n",
            "|    reward             | -5.854045 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 9.64      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 6200       |\n",
            "|    time_elapsed       | 337        |\n",
            "|    total_timesteps    | 31000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6199       |\n",
            "|    policy_loss        | 25.9       |\n",
            "|    reward             | 0.16271242 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 6.97       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 6300      |\n",
            "|    time_elapsed       | 342       |\n",
            "|    total_timesteps    | 31500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6299      |\n",
            "|    policy_loss        | 703       |\n",
            "|    reward             | 6.2977433 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 342       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 6400      |\n",
            "|    time_elapsed       | 347       |\n",
            "|    total_timesteps    | 32000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6399      |\n",
            "|    policy_loss        | 34.7      |\n",
            "|    reward             | 1.4634216 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1         |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 6500       |\n",
            "|    time_elapsed       | 353        |\n",
            "|    total_timesteps    | 32500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6499       |\n",
            "|    policy_loss        | -12.8      |\n",
            "|    reward             | -2.9290671 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.5        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 6600       |\n",
            "|    time_elapsed       | 358        |\n",
            "|    total_timesteps    | 33000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 6599       |\n",
            "|    policy_loss        | 2.19       |\n",
            "|    reward             | -0.7473629 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 2.77       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 6700      |\n",
            "|    time_elapsed       | 364       |\n",
            "|    total_timesteps    | 33500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -0.000395 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6699      |\n",
            "|    policy_loss        | -702      |\n",
            "|    reward             | -6.494263 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 326       |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 91        |\n",
            "|    iterations         | 6800      |\n",
            "|    time_elapsed       | 369       |\n",
            "|    total_timesteps    | 34000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6799      |\n",
            "|    policy_loss        | -234      |\n",
            "|    reward             | 1.4521734 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 36        |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 6900      |\n",
            "|    time_elapsed       | 374       |\n",
            "|    total_timesteps    | 34500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 6899      |\n",
            "|    policy_loss        | -134      |\n",
            "|    reward             | 0.9579974 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 14.8      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 91          |\n",
            "|    iterations         | 7000        |\n",
            "|    time_elapsed       | 380         |\n",
            "|    total_timesteps    | 35000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | -2.38e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 6999        |\n",
            "|    policy_loss        | 40.2        |\n",
            "|    reward             | -0.55972314 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 1.19        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 7100      |\n",
            "|    time_elapsed       | 385       |\n",
            "|    total_timesteps    | 35500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7099      |\n",
            "|    policy_loss        | 8.21      |\n",
            "|    reward             | 1.543814  |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 0.731     |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 91         |\n",
            "|    iterations         | 7200       |\n",
            "|    time_elapsed       | 391        |\n",
            "|    total_timesteps    | 36000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | -1.19e-07  |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7199       |\n",
            "|    policy_loss        | -293       |\n",
            "|    reward             | -1.1853348 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 45         |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 7300        |\n",
            "|    time_elapsed       | 396         |\n",
            "|    total_timesteps    | 36500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42         |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 7299        |\n",
            "|    policy_loss        | 81.6        |\n",
            "|    reward             | 0.045460172 |\n",
            "|    std                | 1.03        |\n",
            "|    value_loss         | 10.4        |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 7400       |\n",
            "|    time_elapsed       | 401        |\n",
            "|    total_timesteps    | 37000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7399       |\n",
            "|    policy_loss        | 507        |\n",
            "|    reward             | -15.030528 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 192        |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 7500       |\n",
            "|    time_elapsed       | 407        |\n",
            "|    total_timesteps    | 37500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7499       |\n",
            "|    policy_loss        | 470        |\n",
            "|    reward             | -14.690647 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 195        |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 7600      |\n",
            "|    time_elapsed       | 412       |\n",
            "|    total_timesteps    | 38000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7599      |\n",
            "|    policy_loss        | -67.1     |\n",
            "|    reward             | 1.5536289 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 2.47      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 7700      |\n",
            "|    time_elapsed       | 418       |\n",
            "|    total_timesteps    | 38500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7699      |\n",
            "|    policy_loss        | -38.6     |\n",
            "|    reward             | 2.0556235 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 1.22      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 7800       |\n",
            "|    time_elapsed       | 423        |\n",
            "|    total_timesteps    | 39000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7799       |\n",
            "|    policy_loss        | -82.8      |\n",
            "|    reward             | 0.35462773 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 5.08       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 7900      |\n",
            "|    time_elapsed       | 427       |\n",
            "|    total_timesteps    | 39500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 7899      |\n",
            "|    policy_loss        | 318       |\n",
            "|    reward             | 2.7938344 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 65.9      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 8000       |\n",
            "|    time_elapsed       | 434        |\n",
            "|    total_timesteps    | 40000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 7999       |\n",
            "|    policy_loss        | -37.3      |\n",
            "|    reward             | 0.93123585 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 13.8       |\n",
            "--------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 8100      |\n",
            "|    time_elapsed       | 438       |\n",
            "|    total_timesteps    | 40500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42       |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8099      |\n",
            "|    policy_loss        | -200      |\n",
            "|    reward             | 13.007109 |\n",
            "|    std                | 1.03      |\n",
            "|    value_loss         | 39.2      |\n",
            "-------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 8200       |\n",
            "|    time_elapsed       | 445        |\n",
            "|    total_timesteps    | 41000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 5.96e-08   |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8199       |\n",
            "|    policy_loss        | 16.5       |\n",
            "|    reward             | 0.30921218 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 0.602      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 8300       |\n",
            "|    time_elapsed       | 449        |\n",
            "|    total_timesteps    | 41500      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42        |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 8299       |\n",
            "|    policy_loss        | 39         |\n",
            "|    reward             | -2.2390645 |\n",
            "|    std                | 1.03       |\n",
            "|    value_loss         | 1.3        |\n",
            "--------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 8400        |\n",
            "|    time_elapsed       | 454         |\n",
            "|    total_timesteps    | 42000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | -1.19e-07   |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8399        |\n",
            "|    policy_loss        | -114        |\n",
            "|    reward             | -0.82526857 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 8.01        |\n",
            "---------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 8500        |\n",
            "|    time_elapsed       | 460         |\n",
            "|    total_timesteps    | 42500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 8499        |\n",
            "|    policy_loss        | -249        |\n",
            "|    reward             | -0.96933323 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 37.9        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 8600      |\n",
            "|    time_elapsed       | 465       |\n",
            "|    total_timesteps    | 43000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8599      |\n",
            "|    policy_loss        | 353       |\n",
            "|    reward             | -8.959472 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 82.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 8700      |\n",
            "|    time_elapsed       | 471       |\n",
            "|    total_timesteps    | 43500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0.028     |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8699      |\n",
            "|    policy_loss        | 1.94      |\n",
            "|    reward             | 0.6782614 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.51      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 8800      |\n",
            "|    time_elapsed       | 476       |\n",
            "|    total_timesteps    | 44000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8799      |\n",
            "|    policy_loss        | -47.9     |\n",
            "|    reward             | 1.137793  |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.09      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 8900      |\n",
            "|    time_elapsed       | 481       |\n",
            "|    total_timesteps    | 44500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8899      |\n",
            "|    policy_loss        | 50.1      |\n",
            "|    reward             | -2.149854 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.87      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 9000      |\n",
            "|    time_elapsed       | 487       |\n",
            "|    total_timesteps    | 45000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 8999      |\n",
            "|    policy_loss        | 25.5      |\n",
            "|    reward             | 0.6270617 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.69      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 9100        |\n",
            "|    time_elapsed       | 492         |\n",
            "|    total_timesteps    | 45500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9099        |\n",
            "|    policy_loss        | 15.3        |\n",
            "|    reward             | -0.03151464 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 1.11        |\n",
            "---------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 9200      |\n",
            "|    time_elapsed       | 498       |\n",
            "|    total_timesteps    | 46000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | 5.96e-08  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9199      |\n",
            "|    policy_loss        | 95        |\n",
            "|    reward             | 4.3043857 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 11.4      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 9300      |\n",
            "|    time_elapsed       | 503       |\n",
            "|    total_timesteps    | 46500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.1     |\n",
            "|    explained_variance | -1.19e-07 |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9299      |\n",
            "|    policy_loss        | -26       |\n",
            "|    reward             | 1.1215013 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 2.02      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 9400        |\n",
            "|    time_elapsed       | 508         |\n",
            "|    total_timesteps    | 47000       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.1       |\n",
            "|    explained_variance | 1.19e-07    |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9399        |\n",
            "|    policy_loss        | 272         |\n",
            "|    reward             | -0.26962775 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 44.9        |\n",
            "---------------------------------------\n",
            "------------------------------------\n",
            "| time/                 |          |\n",
            "|    fps                | 92       |\n",
            "|    iterations         | 9500     |\n",
            "|    time_elapsed       | 514      |\n",
            "|    total_timesteps    | 47500    |\n",
            "| train/                |          |\n",
            "|    entropy_loss       | -42.1    |\n",
            "|    explained_variance | 5.96e-08 |\n",
            "|    learning_rate      | 0.0007   |\n",
            "|    n_updates          | 9499     |\n",
            "|    policy_loss        | 247      |\n",
            "|    reward             | 0.394642 |\n",
            "|    std                | 1.04     |\n",
            "|    value_loss         | 41.4     |\n",
            "------------------------------------\n",
            "----------------------------------------\n",
            "| time/                 |              |\n",
            "|    fps                | 92           |\n",
            "|    iterations         | 9600         |\n",
            "|    time_elapsed       | 519          |\n",
            "|    total_timesteps    | 48000        |\n",
            "| train/                |              |\n",
            "|    entropy_loss       | -42.2        |\n",
            "|    explained_variance | -1.19e-07    |\n",
            "|    learning_rate      | 0.0007       |\n",
            "|    n_updates          | 9599         |\n",
            "|    policy_loss        | -39.5        |\n",
            "|    reward             | -0.025834175 |\n",
            "|    std                | 1.04         |\n",
            "|    value_loss         | 3.69         |\n",
            "----------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 9700      |\n",
            "|    time_elapsed       | 525       |\n",
            "|    total_timesteps    | 48500     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 0         |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9699      |\n",
            "|    policy_loss        | -11.3     |\n",
            "|    reward             | 3.6404629 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 1.31      |\n",
            "-------------------------------------\n",
            "-------------------------------------\n",
            "| time/                 |           |\n",
            "|    fps                | 92        |\n",
            "|    iterations         | 9800      |\n",
            "|    time_elapsed       | 530       |\n",
            "|    total_timesteps    | 49000     |\n",
            "| train/                |           |\n",
            "|    entropy_loss       | -42.2     |\n",
            "|    explained_variance | 1.19e-07  |\n",
            "|    learning_rate      | 0.0007    |\n",
            "|    n_updates          | 9799      |\n",
            "|    policy_loss        | 44.4      |\n",
            "|    reward             | 1.2228005 |\n",
            "|    std                | 1.04      |\n",
            "|    value_loss         | 42.8      |\n",
            "-------------------------------------\n",
            "---------------------------------------\n",
            "| time/                 |             |\n",
            "|    fps                | 92          |\n",
            "|    iterations         | 9900        |\n",
            "|    time_elapsed       | 535         |\n",
            "|    total_timesteps    | 49500       |\n",
            "| train/                |             |\n",
            "|    entropy_loss       | -42.3       |\n",
            "|    explained_variance | 0           |\n",
            "|    learning_rate      | 0.0007      |\n",
            "|    n_updates          | 9899        |\n",
            "|    policy_loss        | -11.1       |\n",
            "|    reward             | -0.42648575 |\n",
            "|    std                | 1.04        |\n",
            "|    value_loss         | 0.226       |\n",
            "---------------------------------------\n",
            "--------------------------------------\n",
            "| time/                 |            |\n",
            "|    fps                | 92         |\n",
            "|    iterations         | 10000      |\n",
            "|    time_elapsed       | 541        |\n",
            "|    total_timesteps    | 50000      |\n",
            "| train/                |            |\n",
            "|    entropy_loss       | -42.4      |\n",
            "|    explained_variance | 0          |\n",
            "|    learning_rate      | 0.0007     |\n",
            "|    n_updates          | 9999       |\n",
            "|    policy_loss        | -9.54      |\n",
            "|    reward             | 0.28997573 |\n",
            "|    std                | 1.05       |\n",
            "|    value_loss         | 1.75       |\n",
            "--------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_a2c = agent.train_model(model=model_a2c,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zjCWfgsg3sVa"
      },
      "outputs": [],
      "source": [
        "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRiOtrywfAo1"
      },
      "source": [
        "### Agent 2: DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2YadjfnLwgt",
        "outputId": "6bf6f1f6-2fc4-4f45-e4ae-b583e42b5042"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/ddpg\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "model_ddpg = agent.get_model(\"ddpg\")\n",
        "\n",
        "if if_using_ddpg:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ddpg'\n",
        "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ddpg.set_logger(new_logger_ddpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCDa78rqfO_a",
        "outputId": "312c339e-d7c0-43d8-c93c-39ca006e98eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 2892, episode: 20\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4672701.93\n",
            "total_reward: 3672701.93\n",
            "total_cost: 4620.46\n",
            "total_trades: 42017\n",
            "Sharpe: 0.832\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 179       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 168       |\n",
            "|    critic_loss     | 18.6      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 3.3875988 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 358       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 100       |\n",
            "|    critic_loss     | 12.5      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 3.3875988 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 30\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5059437.02\n",
            "total_reward: 4059437.02\n",
            "total_cost: 999.00\n",
            "total_trades: 40458\n",
            "Sharpe: 0.799\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 538       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 50.6      |\n",
            "|    critic_loss     | 8.9       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 3.3875988 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 64        |\n",
            "|    time_elapsed    | 717       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 23.5      |\n",
            "|    critic_loss     | 4.42      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 3.3875988 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ddpg = agent.train_model(model=model_ddpg,\n",
        "                             tb_log_name='ddpg',\n",
        "                             total_timesteps=50000) if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ne6M2R-WvrUQ"
      },
      "outputs": [],
      "source": [
        "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gDkU-j-fCmZ"
      },
      "source": [
        "### Agent 3: PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5D5PFUhMzSV",
        "outputId": "f346a918-ed6c-4e72-d01a-9920d1cb76df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
            "Using cuda device\n",
            "Logging to results/ppo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "PPO_PARAMS = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
        "\n",
        "if if_using_ppo:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/ppo'\n",
        "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_ppo.set_logger(new_logger_ppo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gt8eIQKYM4G3",
        "outputId": "545561a7-d328-4f01-ed6d-d323bc80e553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------\n",
            "| time/              |             |\n",
            "|    fps             | 98          |\n",
            "|    iterations      | 1           |\n",
            "|    time_elapsed    | 20          |\n",
            "|    total_timesteps | 2048        |\n",
            "| train/             |             |\n",
            "|    reward          | -0.13087232 |\n",
            "------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 41          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014776676 |\n",
            "|    clip_fraction        | 0.216       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | -0.0132     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 5.75        |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0313     |\n",
            "|    reward               | 1.0909504   |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 62          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018070433 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.2       |\n",
            "|    explained_variance   | 0.00187     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.1        |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0176     |\n",
            "|    reward               | -1.4348191  |\n",
            "|    std                  | 1           |\n",
            "|    value_loss           | 54          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 83          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019182725 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.3       |\n",
            "|    explained_variance   | -0.00371    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.6        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0188     |\n",
            "|    reward               | 3.8600285   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 47.8        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 40\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4102403.39\n",
            "total_reward: 3102403.39\n",
            "total_cost: 322286.67\n",
            "total_trades: 80503\n",
            "Sharpe: 0.804\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 103         |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014266701 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.0068      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.4        |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0225     |\n",
            "|    reward               | 3.700708    |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 21.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 124         |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022270374 |\n",
            "|    clip_fraction        | 0.232       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | -0.0216     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16.9        |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    reward               | 1.5445446   |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 65.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 143         |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019399878 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.4       |\n",
            "|    explained_variance   | 0.00936     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 40.8        |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0213     |\n",
            "|    reward               | 0.18938014  |\n",
            "|    std                  | 1.01        |\n",
            "|    value_loss           | 61.6        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 164        |\n",
            "|    total_timesteps      | 16384      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01753972 |\n",
            "|    clip_fraction        | 0.171      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | -0.00238   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 19.9       |\n",
            "|    n_updates            | 70         |\n",
            "|    policy_gradient_loss | -0.0186    |\n",
            "|    reward               | 0.54314744 |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 37.4       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 9          |\n",
            "|    time_elapsed         | 184        |\n",
            "|    total_timesteps      | 18432      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01446547 |\n",
            "|    clip_fraction        | 0.157      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.5      |\n",
            "|    explained_variance   | -0.0212    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 34         |\n",
            "|    n_updates            | 80         |\n",
            "|    policy_gradient_loss | -0.0204    |\n",
            "|    reward               | 0.7970209  |\n",
            "|    std                  | 1.01       |\n",
            "|    value_loss           | 82.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 205         |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019993756 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | -0.00703    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 52.8        |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    reward               | 0.19893238  |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 173         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 225         |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018808402 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.6       |\n",
            "|    explained_variance   | 0.00376     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 142         |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0175     |\n",
            "|    reward               | 2.8818364   |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 207         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 246         |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021838162 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.0243     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 16          |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    reward               | -0.11720421 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 31.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 266         |\n",
            "|    total_timesteps      | 26624       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021468483 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.7       |\n",
            "|    explained_variance   | -0.00431    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 112         |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0172     |\n",
            "|    reward               | -0.21451859 |\n",
            "|    std                  | 1.02        |\n",
            "|    value_loss           | 141         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 14         |\n",
            "|    time_elapsed         | 286        |\n",
            "|    total_timesteps      | 28672      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01830654 |\n",
            "|    clip_fraction        | 0.194      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.8      |\n",
            "|    explained_variance   | 0.0122     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 25.3       |\n",
            "|    n_updates            | 130        |\n",
            "|    policy_gradient_loss | -0.0191    |\n",
            "|    reward               | -2.07078   |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 116        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 15         |\n",
            "|    time_elapsed         | 308        |\n",
            "|    total_timesteps      | 30720      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03240667 |\n",
            "|    clip_fraction        | 0.316      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -41.8      |\n",
            "|    explained_variance   | -0.0222    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 24.6       |\n",
            "|    n_updates            | 140        |\n",
            "|    policy_gradient_loss | -0.0163    |\n",
            "|    reward               | 5.154183   |\n",
            "|    std                  | 1.02       |\n",
            "|    value_loss           | 44.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 327         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.01588949  |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 0.00379     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 74.2        |\n",
            "|    n_updates            | 150         |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    reward               | -0.14392227 |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 349         |\n",
            "|    total_timesteps      | 34816       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018665746 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | 7.77e-05    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 29.4        |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0134     |\n",
            "|    reward               | 0.4565778   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 97.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 369         |\n",
            "|    total_timesteps      | 36864       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015541421 |\n",
            "|    clip_fraction        | 0.15        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -41.9       |\n",
            "|    explained_variance   | -0.00373    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 119         |\n",
            "|    n_updates            | 170         |\n",
            "|    policy_gradient_loss | -0.0126     |\n",
            "|    reward               | 1.3885818   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 148         |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 50\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4012970.07\n",
            "total_reward: 3012970.07\n",
            "total_cost: 256306.32\n",
            "total_trades: 75633\n",
            "Sharpe: 0.753\n",
            "=================================\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 99            |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 390           |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.027788896   |\n",
            "|    clip_fraction        | 0.25          |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -42           |\n",
            "|    explained_variance   | 0.00688       |\n",
            "|    learning_rate        | 0.00025       |\n",
            "|    loss                 | 11            |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -0.0106       |\n",
            "|    reward               | -0.0012143902 |\n",
            "|    std                  | 1.03          |\n",
            "|    value_loss           | 25.5          |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 410         |\n",
            "|    total_timesteps      | 40960       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026103403 |\n",
            "|    clip_fraction        | 0.187       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42         |\n",
            "|    explained_variance   | 0.0249      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26.8        |\n",
            "|    n_updates            | 190         |\n",
            "|    policy_gradient_loss | -0.00326    |\n",
            "|    reward               | -1.073638   |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 119         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 431         |\n",
            "|    total_timesteps      | 43008       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028699841 |\n",
            "|    clip_fraction        | 0.263       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0105      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.2        |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | -7.1719003  |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 452         |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039514728 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | -0.0116     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.3        |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.0192     |\n",
            "|    reward               | 2.263331    |\n",
            "|    std                  | 1.03        |\n",
            "|    value_loss           | 32.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 472         |\n",
            "|    total_timesteps      | 47104       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027514888 |\n",
            "|    clip_fraction        | 0.285       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.1       |\n",
            "|    explained_variance   | 0.0225      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 38.6        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    reward               | 0.940099    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 88.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 494         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024374478 |\n",
            "|    clip_fraction        | 0.203       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | 0.0154      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 61.2        |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0112     |\n",
            "|    reward               | 8.045275    |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 119         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 515         |\n",
            "|    total_timesteps      | 51200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033369392 |\n",
            "|    clip_fraction        | 0.317       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.2       |\n",
            "|    explained_variance   | -0.00461    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32.7        |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0142     |\n",
            "|    reward               | -0.6725708  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 137         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 536         |\n",
            "|    total_timesteps      | 53248       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024074549 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.3       |\n",
            "|    explained_variance   | 0.0733      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.1        |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    reward               | 0.40824714  |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 24.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 556         |\n",
            "|    total_timesteps      | 55296       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023305506 |\n",
            "|    clip_fraction        | 0.241       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0177      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 56.3        |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    reward               | -0.16341843 |\n",
            "|    std                  | 1.04        |\n",
            "|    value_loss           | 118         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 578         |\n",
            "|    total_timesteps      | 57344       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025509471 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.4       |\n",
            "|    explained_variance   | 0.0128      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 72.1        |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | -1.5696696  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 103         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 598         |\n",
            "|    total_timesteps      | 59392       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025221696 |\n",
            "|    clip_fraction        | 0.248       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.098       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.5        |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | 0.1935365   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 35.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 619         |\n",
            "|    total_timesteps      | 61440       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.030695401 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.5       |\n",
            "|    explained_variance   | 0.0254      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 102         |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    reward               | 0.5627918   |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 133         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 31         |\n",
            "|    time_elapsed         | 640        |\n",
            "|    total_timesteps      | 63488      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02062867 |\n",
            "|    clip_fraction        | 0.187      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -42.5      |\n",
            "|    explained_variance   | 0.00976    |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 22.9       |\n",
            "|    n_updates            | 300        |\n",
            "|    policy_gradient_loss | -0.0145    |\n",
            "|    reward               | 3.9385889  |\n",
            "|    std                  | 1.05       |\n",
            "|    value_loss           | 69.5       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 661         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.026818197 |\n",
            "|    clip_fraction        | 0.282       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.00785     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 18          |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    reward               | -1.9609575  |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 35.2        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 60\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 5398697.54\n",
            "total_reward: 4398697.54\n",
            "total_cost: 285097.95\n",
            "total_trades: 76513\n",
            "Sharpe: 0.924\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 682         |\n",
            "|    total_timesteps      | 67584       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023236893 |\n",
            "|    clip_fraction        | 0.247       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.6       |\n",
            "|    explained_variance   | 0.0446      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 43.6        |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    reward               | -0.30074334 |\n",
            "|    std                  | 1.05        |\n",
            "|    value_loss           | 60.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 702         |\n",
            "|    total_timesteps      | 69632       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025110256 |\n",
            "|    clip_fraction        | 0.277       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0442      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 31.1        |\n",
            "|    n_updates            | 330         |\n",
            "|    policy_gradient_loss | -0.0109     |\n",
            "|    reward               | 0.9998566   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 119         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 724         |\n",
            "|    total_timesteps      | 71680       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014995672 |\n",
            "|    clip_fraction        | 0.192       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.7       |\n",
            "|    explained_variance   | 0.0158      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 27.2        |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.00756    |\n",
            "|    reward               | 1.0950619   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 59          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 744         |\n",
            "|    total_timesteps      | 73728       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024335096 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.166       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.3        |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    reward               | -4.7095356  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 17.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 765         |\n",
            "|    total_timesteps      | 75776       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021726325 |\n",
            "|    clip_fraction        | 0.202       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.00512     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 39.5        |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0175     |\n",
            "|    reward               | -0.60313994 |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 83.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 786         |\n",
            "|    total_timesteps      | 77824       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024586622 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | -0.00331    |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 90.8        |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.00686    |\n",
            "|    reward               | -4.503702   |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 120         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 806         |\n",
            "|    total_timesteps      | 79872       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015450961 |\n",
            "|    clip_fraction        | 0.16        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.8       |\n",
            "|    explained_variance   | 0.0707      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 26          |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    reward               | -2.8144662  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 47.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 828         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023470432 |\n",
            "|    clip_fraction        | 0.188       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.0501      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.3        |\n",
            "|    n_updates            | 390         |\n",
            "|    policy_gradient_loss | -0.0125     |\n",
            "|    reward               | -0.5737354  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 58.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 848         |\n",
            "|    total_timesteps      | 83968       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028517032 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -42.9       |\n",
            "|    explained_variance   | 0.064       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.8        |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.00621    |\n",
            "|    reward               | 0.42865846  |\n",
            "|    std                  | 1.06        |\n",
            "|    value_loss           | 99          |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 98           |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 869          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.021946874  |\n",
            "|    clip_fraction        | 0.271        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -42.9        |\n",
            "|    explained_variance   | 0.0141       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 69.9         |\n",
            "|    n_updates            | 410          |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    reward               | -0.011037302 |\n",
            "|    std                  | 1.06         |\n",
            "|    value_loss           | 103          |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 889         |\n",
            "|    total_timesteps      | 88064       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.05545982  |\n",
            "|    clip_fraction        | 0.327       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.322       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 7.74        |\n",
            "|    n_updates            | 420         |\n",
            "|    policy_gradient_loss | -0.00452    |\n",
            "|    reward               | -0.05601795 |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 16.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 910         |\n",
            "|    total_timesteps      | 90112       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038122617 |\n",
            "|    clip_fraction        | 0.335       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43         |\n",
            "|    explained_variance   | 0.0632      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.9        |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.00609    |\n",
            "|    reward               | 0.22314247  |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 86.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 930         |\n",
            "|    total_timesteps      | 92160       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034184646 |\n",
            "|    clip_fraction        | 0.329       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.1       |\n",
            "|    explained_variance   | 0.0269      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 54.5        |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.00436    |\n",
            "|    reward               | -1.527011   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 92.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 98          |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 951         |\n",
            "|    total_timesteps      | 94208       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036712464 |\n",
            "|    clip_fraction        | 0.295       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.2       |\n",
            "|    explained_variance   | 0.0592      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 12          |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.00676    |\n",
            "|    reward               | -4.940852   |\n",
            "|    std                  | 1.07        |\n",
            "|    value_loss           | 29          |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 70\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3632677.03\n",
            "total_reward: 2632677.03\n",
            "total_cost: 262817.36\n",
            "total_trades: 74475\n",
            "Sharpe: 0.732\n",
            "=================================\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 47         |\n",
            "|    time_elapsed         | 971        |\n",
            "|    total_timesteps      | 96256      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02593024 |\n",
            "|    clip_fraction        | 0.287      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.2      |\n",
            "|    explained_variance   | 0.0518     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 55.8       |\n",
            "|    n_updates            | 460        |\n",
            "|    policy_gradient_loss | -0.00914   |\n",
            "|    reward               | 0.8856009  |\n",
            "|    std                  | 1.07       |\n",
            "|    value_loss           | 88.8       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 992        |\n",
            "|    total_timesteps      | 98304      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03927914 |\n",
            "|    clip_fraction        | 0.239      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.2      |\n",
            "|    explained_variance   | 0.022      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 46.7       |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.00622   |\n",
            "|    reward               | 12.262447  |\n",
            "|    std                  | 1.08       |\n",
            "|    value_loss           | 93.3       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 1013        |\n",
            "|    total_timesteps      | 100352      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036273234 |\n",
            "|    clip_fraction        | 0.273       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.0214      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 32          |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.00497    |\n",
            "|    reward               | -1.8565385  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 81.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 1033        |\n",
            "|    total_timesteps      | 102400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034119383 |\n",
            "|    clip_fraction        | 0.3         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.3       |\n",
            "|    explained_variance   | 0.149       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 9.76        |\n",
            "|    n_updates            | 490         |\n",
            "|    policy_gradient_loss | -0.0103     |\n",
            "|    reward               | -4.3740726  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 23.4        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1054        |\n",
            "|    total_timesteps      | 104448      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019344144 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.4       |\n",
            "|    explained_variance   | 0.0835      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 51.2        |\n",
            "|    n_updates            | 500         |\n",
            "|    policy_gradient_loss | -0.00232    |\n",
            "|    reward               | -0.3506656  |\n",
            "|    std                  | 1.08        |\n",
            "|    value_loss           | 97.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1074        |\n",
            "|    total_timesteps      | 106496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.049651347 |\n",
            "|    clip_fraction        | 0.373       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.0134      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 25.8        |\n",
            "|    n_updates            | 510         |\n",
            "|    policy_gradient_loss | -0.00124    |\n",
            "|    reward               | -7.107237   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 118         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 1095        |\n",
            "|    total_timesteps      | 108544      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060489915 |\n",
            "|    clip_fraction        | 0.351       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.5       |\n",
            "|    explained_variance   | 0.127       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 11.3        |\n",
            "|    n_updates            | 520         |\n",
            "|    policy_gradient_loss | -0.00618    |\n",
            "|    reward               | 1.5288099   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 27.9        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 54         |\n",
            "|    time_elapsed         | 1114       |\n",
            "|    total_timesteps      | 110592     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03481513 |\n",
            "|    clip_fraction        | 0.323      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.6      |\n",
            "|    explained_variance   | 0.0824     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 42.5       |\n",
            "|    n_updates            | 530        |\n",
            "|    policy_gradient_loss | -0.0105    |\n",
            "|    reward               | 0.71376187 |\n",
            "|    std                  | 1.09       |\n",
            "|    value_loss           | 107        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1136        |\n",
            "|    total_timesteps      | 112640      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028556515 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.0731      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 66          |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.00944    |\n",
            "|    reward               | 4.5014763   |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 124         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1155        |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033118267 |\n",
            "|    clip_fraction        | 0.298       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.7       |\n",
            "|    explained_variance   | 0.171       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 28.2        |\n",
            "|    n_updates            | 550         |\n",
            "|    policy_gradient_loss | -0.00711    |\n",
            "|    reward               | 0.92515355  |\n",
            "|    std                  | 1.09        |\n",
            "|    value_loss           | 44.6        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1177        |\n",
            "|    total_timesteps      | 116736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033751976 |\n",
            "|    clip_fraction        | 0.313       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.111       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 57.3        |\n",
            "|    n_updates            | 560         |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    reward               | -0.10734636 |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 113         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 58          |\n",
            "|    time_elapsed         | 1196        |\n",
            "|    total_timesteps      | 118784      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024775106 |\n",
            "|    clip_fraction        | 0.24        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -43.8       |\n",
            "|    explained_variance   | 0.0907      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 66.3        |\n",
            "|    n_updates            | 570         |\n",
            "|    policy_gradient_loss | -0.00556    |\n",
            "|    reward               | 0.6758292   |\n",
            "|    std                  | 1.1         |\n",
            "|    value_loss           | 131         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 59         |\n",
            "|    time_elapsed         | 1217       |\n",
            "|    total_timesteps      | 120832     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04141365 |\n",
            "|    clip_fraction        | 0.318      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -43.9      |\n",
            "|    explained_variance   | 0.0469     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 59.6       |\n",
            "|    n_updates            | 580        |\n",
            "|    policy_gradient_loss | -0.00908   |\n",
            "|    reward               | -1.2205412 |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 117        |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 60         |\n",
            "|    time_elapsed         | 1238       |\n",
            "|    total_timesteps      | 122880     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03505557 |\n",
            "|    clip_fraction        | 0.333      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44        |\n",
            "|    explained_variance   | 0.228      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 12.8       |\n",
            "|    n_updates            | 590        |\n",
            "|    policy_gradient_loss | -0.00905   |\n",
            "|    reward               | 1.1933621  |\n",
            "|    std                  | 1.1        |\n",
            "|    value_loss           | 22.3       |\n",
            "----------------------------------------\n",
            "day: 2892, episode: 80\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6299587.46\n",
            "total_reward: 5299587.46\n",
            "total_cost: 272994.92\n",
            "total_trades: 75841\n",
            "Sharpe: 0.979\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1258        |\n",
            "|    total_timesteps      | 124928      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033297215 |\n",
            "|    clip_fraction        | 0.233       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44         |\n",
            "|    explained_variance   | 0.0673      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 41          |\n",
            "|    n_updates            | 600         |\n",
            "|    policy_gradient_loss | -0.00597    |\n",
            "|    reward               | 1.005013    |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 135         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 62          |\n",
            "|    time_elapsed         | 1279        |\n",
            "|    total_timesteps      | 126976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029969256 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.1       |\n",
            "|    explained_variance   | 0.0724      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 66.3        |\n",
            "|    n_updates            | 610         |\n",
            "|    policy_gradient_loss | -0.00694    |\n",
            "|    reward               | 3.5787735   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 183         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1299        |\n",
            "|    total_timesteps      | 129024      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034901354 |\n",
            "|    clip_fraction        | 0.312       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0345      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 24          |\n",
            "|    n_updates            | 620         |\n",
            "|    policy_gradient_loss | -0.00144    |\n",
            "|    reward               | 3.5796044   |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 46.3        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1320        |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029693376 |\n",
            "|    clip_fraction        | 0.267       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.2       |\n",
            "|    explained_variance   | 0.0776      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 43.5        |\n",
            "|    n_updates            | 630         |\n",
            "|    policy_gradient_loss | -0.0104     |\n",
            "|    reward               | -1.1268908  |\n",
            "|    std                  | 1.11        |\n",
            "|    value_loss           | 97.9        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1341        |\n",
            "|    total_timesteps      | 133120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038567107 |\n",
            "|    clip_fraction        | 0.315       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.0734      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 89.8        |\n",
            "|    n_updates            | 640         |\n",
            "|    policy_gradient_loss | -0.0121     |\n",
            "|    reward               | -1.2149757  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 110         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 66          |\n",
            "|    time_elapsed         | 1362        |\n",
            "|    total_timesteps      | 135168      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035967402 |\n",
            "|    clip_fraction        | 0.235       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.3       |\n",
            "|    explained_variance   | 0.0175      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 51.6        |\n",
            "|    n_updates            | 650         |\n",
            "|    policy_gradient_loss | -0.00118    |\n",
            "|    reward               | 4.3143544   |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 135         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 67         |\n",
            "|    time_elapsed         | 1382       |\n",
            "|    total_timesteps      | 137216     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04392977 |\n",
            "|    clip_fraction        | 0.34       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.4      |\n",
            "|    explained_variance   | 0.23       |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 9.05       |\n",
            "|    n_updates            | 660        |\n",
            "|    policy_gradient_loss | 0.000806   |\n",
            "|    reward               | -2.1118155 |\n",
            "|    std                  | 1.12       |\n",
            "|    value_loss           | 16.8       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 68          |\n",
            "|    time_elapsed         | 1403        |\n",
            "|    total_timesteps      | 139264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022780951 |\n",
            "|    clip_fraction        | 0.182       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.4       |\n",
            "|    explained_variance   | 0.0897      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 42.4        |\n",
            "|    n_updates            | 670         |\n",
            "|    policy_gradient_loss | -0.00929    |\n",
            "|    reward               | 0.107373364 |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 149         |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 99        |\n",
            "|    iterations           | 69        |\n",
            "|    time_elapsed         | 1423      |\n",
            "|    total_timesteps      | 141312    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.037927  |\n",
            "|    clip_fraction        | 0.27      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -44.5     |\n",
            "|    explained_variance   | 0.0753    |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 38.9      |\n",
            "|    n_updates            | 680       |\n",
            "|    policy_gradient_loss | -0.00644  |\n",
            "|    reward               | 1.2955941 |\n",
            "|    std                  | 1.12      |\n",
            "|    value_loss           | 90.5      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1444        |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.039086197 |\n",
            "|    clip_fraction        | 0.296       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.5       |\n",
            "|    explained_variance   | 0.0931      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 13.3        |\n",
            "|    n_updates            | 690         |\n",
            "|    policy_gradient_loss | -0.0102     |\n",
            "|    reward               | -0.1696883  |\n",
            "|    std                  | 1.12        |\n",
            "|    value_loss           | 32.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 99           |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 1465         |\n",
            "|    total_timesteps      | 145408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.028489511  |\n",
            "|    clip_fraction        | 0.248        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -44.6        |\n",
            "|    explained_variance   | 0.0493       |\n",
            "|    learning_rate        | 0.00025      |\n",
            "|    loss                 | 33.5         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.0161      |\n",
            "|    reward               | -0.028027317 |\n",
            "|    std                  | 1.13         |\n",
            "|    value_loss           | 119          |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 72         |\n",
            "|    time_elapsed         | 1485       |\n",
            "|    total_timesteps      | 147456     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03229759 |\n",
            "|    clip_fraction        | 0.255      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.6      |\n",
            "|    explained_variance   | -0.00593   |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 21.9       |\n",
            "|    n_updates            | 710        |\n",
            "|    policy_gradient_loss | -0.00675   |\n",
            "|    reward               | -13.629854 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 40.2       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 73         |\n",
            "|    time_elapsed         | 1506       |\n",
            "|    total_timesteps      | 149504     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04141922 |\n",
            "|    clip_fraction        | 0.377      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.7      |\n",
            "|    explained_variance   | 0.0461     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 16.9       |\n",
            "|    n_updates            | 720        |\n",
            "|    policy_gradient_loss | -0.00507   |\n",
            "|    reward               | -1.5382438 |\n",
            "|    std                  | 1.13       |\n",
            "|    value_loss           | 26.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1525        |\n",
            "|    total_timesteps      | 151552      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032973975 |\n",
            "|    clip_fraction        | 0.289       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.7       |\n",
            "|    explained_variance   | 0.217       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 14.5        |\n",
            "|    n_updates            | 730         |\n",
            "|    policy_gradient_loss | -0.0118     |\n",
            "|    reward               | -1.6490744  |\n",
            "|    std                  | 1.13        |\n",
            "|    value_loss           | 24.8        |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 90\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 3212393.09\n",
            "total_reward: 2212393.09\n",
            "total_cost: 271303.53\n",
            "total_trades: 74704\n",
            "Sharpe: 0.663\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1547        |\n",
            "|    total_timesteps      | 153600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028998375 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | 0.108       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.5        |\n",
            "|    n_updates            | 740         |\n",
            "|    policy_gradient_loss | -0.00396    |\n",
            "|    reward               | 1.5356051   |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 153         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1566        |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038520914 |\n",
            "|    clip_fraction        | 0.294       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.8       |\n",
            "|    explained_variance   | -0.0156     |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 30.6        |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.00465    |\n",
            "|    reward               | -2.6187801  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 47.5        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1588        |\n",
            "|    total_timesteps      | 157696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029687217 |\n",
            "|    clip_fraction        | 0.259       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -44.9       |\n",
            "|    explained_variance   | 0.249       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 17.1        |\n",
            "|    n_updates            | 760         |\n",
            "|    policy_gradient_loss | -0.00509    |\n",
            "|    reward               | 0.64970195  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 38.3        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 78         |\n",
            "|    time_elapsed         | 1608       |\n",
            "|    total_timesteps      | 159744     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02014904 |\n",
            "|    clip_fraction        | 0.181      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -44.9      |\n",
            "|    explained_variance   | 0.0918     |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 77.8       |\n",
            "|    n_updates            | 770        |\n",
            "|    policy_gradient_loss | 0.000398   |\n",
            "|    reward               | 2.2353334  |\n",
            "|    std                  | 1.14       |\n",
            "|    value_loss           | 213        |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 79          |\n",
            "|    time_elapsed         | 1629        |\n",
            "|    total_timesteps      | 161792      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035935063 |\n",
            "|    clip_fraction        | 0.237       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.0824      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 129         |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00106    |\n",
            "|    reward               | 0.45825517  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 221         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 80          |\n",
            "|    time_elapsed         | 1649        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.023441788 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45         |\n",
            "|    explained_variance   | 0.118       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 48.5        |\n",
            "|    n_updates            | 790         |\n",
            "|    policy_gradient_loss | -0.00605    |\n",
            "|    reward               | -0.7145837  |\n",
            "|    std                  | 1.14        |\n",
            "|    value_loss           | 90          |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1669        |\n",
            "|    total_timesteps      | 165888      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027749551 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.1       |\n",
            "|    explained_variance   | 0.109       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 137         |\n",
            "|    n_updates            | 800         |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    reward               | -0.12016963 |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 159         |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 82         |\n",
            "|    time_elapsed         | 1690       |\n",
            "|    total_timesteps      | 167936     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03662693 |\n",
            "|    clip_fraction        | 0.354      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.1      |\n",
            "|    explained_variance   | 0.106      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 47.9       |\n",
            "|    n_updates            | 810        |\n",
            "|    policy_gradient_loss | -0.00517   |\n",
            "|    reward               | -0.0111873 |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 68.2       |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 99         |\n",
            "|    iterations           | 83         |\n",
            "|    time_elapsed         | 1710       |\n",
            "|    total_timesteps      | 169984     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03606236 |\n",
            "|    clip_fraction        | 0.266      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -45.2      |\n",
            "|    explained_variance   | 0.105      |\n",
            "|    learning_rate        | 0.00025    |\n",
            "|    loss                 | 31.4       |\n",
            "|    n_updates            | 820        |\n",
            "|    policy_gradient_loss | -0.00342   |\n",
            "|    reward               | 0.9042617  |\n",
            "|    std                  | 1.15       |\n",
            "|    value_loss           | 99.4       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 1731        |\n",
            "|    total_timesteps      | 172032      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.046207692 |\n",
            "|    clip_fraction        | 0.384       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.518       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 8.37        |\n",
            "|    n_updates            | 830         |\n",
            "|    policy_gradient_loss | -0.000446   |\n",
            "|    reward               | 0.4714885   |\n",
            "|    std                  | 1.15        |\n",
            "|    value_loss           | 18.7        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1750        |\n",
            "|    total_timesteps      | 174080      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.04944875  |\n",
            "|    clip_fraction        | 0.357       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.3       |\n",
            "|    explained_variance   | 0.219       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 77.3        |\n",
            "|    n_updates            | 840         |\n",
            "|    policy_gradient_loss | 0.00318     |\n",
            "|    reward               | 0.103401355 |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 102         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1771        |\n",
            "|    total_timesteps      | 176128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034635376 |\n",
            "|    clip_fraction        | 0.291       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.4       |\n",
            "|    explained_variance   | 0.174       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 33.4        |\n",
            "|    n_updates            | 850         |\n",
            "|    policy_gradient_loss | 0.000804    |\n",
            "|    reward               | -0.46862918 |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 117         |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 99        |\n",
            "|    iterations           | 87        |\n",
            "|    time_elapsed         | 1791      |\n",
            "|    total_timesteps      | 178176    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0326644 |\n",
            "|    clip_fraction        | 0.302     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -45.4     |\n",
            "|    explained_variance   | 0.183     |\n",
            "|    learning_rate        | 0.00025   |\n",
            "|    loss                 | 44        |\n",
            "|    n_updates            | 860       |\n",
            "|    policy_gradient_loss | -0.0024   |\n",
            "|    reward               | 2.9813778 |\n",
            "|    std                  | 1.16      |\n",
            "|    value_loss           | 80.2      |\n",
            "---------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1812        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042115465 |\n",
            "|    clip_fraction        | 0.258       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.5       |\n",
            "|    explained_variance   | 0.161       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 65.7        |\n",
            "|    n_updates            | 870         |\n",
            "|    policy_gradient_loss | -0.00535    |\n",
            "|    reward               | -1.2659972  |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 178         |\n",
            "-----------------------------------------\n",
            "day: 2892, episode: 100\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 7239111.34\n",
            "total_reward: 6239111.34\n",
            "total_cost: 301066.98\n",
            "total_trades: 76139\n",
            "Sharpe: 1.010\n",
            "=================================\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 89          |\n",
            "|    time_elapsed         | 1832        |\n",
            "|    total_timesteps      | 182272      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019518448 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.5       |\n",
            "|    explained_variance   | 0.179       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 122         |\n",
            "|    n_updates            | 880         |\n",
            "|    policy_gradient_loss | -0.00731    |\n",
            "|    reward               | -0.16175145 |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 219         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 90          |\n",
            "|    time_elapsed         | 1853        |\n",
            "|    total_timesteps      | 184320      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015541159 |\n",
            "|    clip_fraction        | 0.157       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.6       |\n",
            "|    explained_variance   | 0.0494      |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 122         |\n",
            "|    n_updates            | 890         |\n",
            "|    policy_gradient_loss | -0.00684    |\n",
            "|    reward               | -0.50913817 |\n",
            "|    std                  | 1.16        |\n",
            "|    value_loss           | 198         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 91          |\n",
            "|    time_elapsed         | 1873        |\n",
            "|    total_timesteps      | 186368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032847896 |\n",
            "|    clip_fraction        | 0.245       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.6       |\n",
            "|    explained_variance   | 0.362       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 10.7        |\n",
            "|    n_updates            | 900         |\n",
            "|    policy_gradient_loss | -0.00818    |\n",
            "|    reward               | -0.99602306 |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 22.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1894        |\n",
            "|    total_timesteps      | 188416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038546734 |\n",
            "|    clip_fraction        | 0.3         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.6       |\n",
            "|    explained_variance   | 0.118       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 129         |\n",
            "|    n_updates            | 910         |\n",
            "|    policy_gradient_loss | -5.5e-05    |\n",
            "|    reward               | -1.6851164  |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 197         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 93          |\n",
            "|    time_elapsed         | 1915        |\n",
            "|    total_timesteps      | 190464      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025705371 |\n",
            "|    clip_fraction        | 0.28        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.7       |\n",
            "|    explained_variance   | 0.108       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 175         |\n",
            "|    n_updates            | 920         |\n",
            "|    policy_gradient_loss | -0.00293    |\n",
            "|    reward               | -3.9926305  |\n",
            "|    std                  | 1.17        |\n",
            "|    value_loss           | 309         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1935        |\n",
            "|    total_timesteps      | 192512      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.044991784 |\n",
            "|    clip_fraction        | 0.366       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.8       |\n",
            "|    explained_variance   | 0.391       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 22.1        |\n",
            "|    n_updates            | 930         |\n",
            "|    policy_gradient_loss | -0.00326    |\n",
            "|    reward               | -0.32399765 |\n",
            "|    std                  | 1.18        |\n",
            "|    value_loss           | 50.2        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1956        |\n",
            "|    total_timesteps      | 194560      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025837984 |\n",
            "|    clip_fraction        | 0.251       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.9       |\n",
            "|    explained_variance   | 0.237       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 165         |\n",
            "|    n_updates            | 940         |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    reward               | -1.1085138  |\n",
            "|    std                  | 1.18        |\n",
            "|    value_loss           | 258         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 96          |\n",
            "|    time_elapsed         | 1976        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024962977 |\n",
            "|    clip_fraction        | 0.254       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -45.9       |\n",
            "|    explained_variance   | 0.223       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 110         |\n",
            "|    n_updates            | 950         |\n",
            "|    policy_gradient_loss | 0.00235     |\n",
            "|    reward               | 5.3681993   |\n",
            "|    std                  | 1.18        |\n",
            "|    value_loss           | 278         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 1997        |\n",
            "|    total_timesteps      | 198656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.025590956 |\n",
            "|    clip_fraction        | 0.275       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -46         |\n",
            "|    explained_variance   | 0.12        |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 59.8        |\n",
            "|    n_updates            | 960         |\n",
            "|    policy_gradient_loss | -0.00393    |\n",
            "|    reward               | -0.8454463  |\n",
            "|    std                  | 1.18        |\n",
            "|    value_loss           | 140         |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 99          |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 2017        |\n",
            "|    total_timesteps      | 200704      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034752566 |\n",
            "|    clip_fraction        | 0.262       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -46.1       |\n",
            "|    explained_variance   | 0.187       |\n",
            "|    learning_rate        | 0.00025     |\n",
            "|    loss                 | 133         |\n",
            "|    n_updates            | 970         |\n",
            "|    policy_gradient_loss | -0.0037     |\n",
            "|    reward               | -0.08481723 |\n",
            "|    std                  | 1.19        |\n",
            "|    value_loss           | 191         |\n",
            "-----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=200000) if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "C6AidlWyvwzm"
      },
      "outputs": [],
      "source": [
        "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zpv4S0-fDBv"
      },
      "source": [
        "### Agent 4: TD3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSAHhV4Xc-bh",
        "outputId": "36607b5a-eb58-4075-8500-b012af2de09a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
            "Using cuda device\n",
            "Logging to results/td3\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "TD3_PARAMS = {\"batch_size\": 100,\n",
        "              \"buffer_size\": 1000000,\n",
        "              \"learning_rate\": 0.001}\n",
        "\n",
        "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
        "\n",
        "if if_using_td3:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/td3'\n",
        "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_td3.set_logger(new_logger_td3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSRxNYAxdKpU",
        "outputId": "127c99e2-7a86-4727-a216-53861129780e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "day: 2892, episode: 110\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6415184.26\n",
            "total_reward: 5415184.26\n",
            "total_cost: 1251.45\n",
            "total_trades: 49256\n",
            "Sharpe: 0.959\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 173       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 152       |\n",
            "|    critic_loss     | 986       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 6.1512284 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 349       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 93.1      |\n",
            "|    critic_loss     | 24.4      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 6.1512284 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 523       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 93.4      |\n",
            "|    critic_loss     | 701       |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 6.1512284 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 120\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6415184.26\n",
            "total_reward: 5415184.26\n",
            "total_cost: 1251.45\n",
            "total_trades: 49256\n",
            "Sharpe: 0.959\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 16        |\n",
            "|    fps             | 66        |\n",
            "|    time_elapsed    | 696       |\n",
            "|    total_timesteps | 46288     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 76.1      |\n",
            "|    critic_loss     | 21.2      |\n",
            "|    learning_rate   | 0.001     |\n",
            "|    n_updates       | 46187     |\n",
            "|    reward          | 6.1512284 |\n",
            "----------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_td3 = agent.train_model(model=model_td3,\n",
        "                             tb_log_name='td3',\n",
        "                             total_timesteps=50000) if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "OkJV6V_mv2hw"
      },
      "outputs": [],
      "source": [
        "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr49PotrfG01"
      },
      "source": [
        "### Agent 5: SAC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwOhVjqRkCdM",
        "outputId": "42a1dcf8-b74f-4648-bcae-5f7f42007c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
            "Using cuda device\n",
            "Logging to results/sac\n"
          ]
        }
      ],
      "source": [
        "agent = DRLAgent(env = env_train)\n",
        "SAC_PARAMS = {\n",
        "    \"batch_size\": 128,\n",
        "    \"buffer_size\": 100000,\n",
        "    \"learning_rate\": 0.0001,\n",
        "    \"learning_starts\": 100,\n",
        "    \"ent_coef\": \"auto_0.1\",\n",
        "}\n",
        "\n",
        "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
        "\n",
        "if if_using_sac:\n",
        "  # set up logger\n",
        "  tmp_path = RESULTS_DIR + '/sac'\n",
        "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "  # Set new logger\n",
        "  model_sac.set_logger(new_logger_sac)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8RSdKCckJyH",
        "outputId": "e0330c97-e22b-44b4-8ef5-39ffc3965c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 4         |\n",
            "|    fps             | 48        |\n",
            "|    time_elapsed    | 236       |\n",
            "|    total_timesteps | 11572     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 1.78e+03  |\n",
            "|    critic_loss     | 139       |\n",
            "|    ent_coef        | 0.274     |\n",
            "|    ent_coef_loss   | 121       |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 11471     |\n",
            "|    reward          | 10.653123 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 130\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 6671036.70\n",
            "total_reward: 5671036.70\n",
            "total_cost: 234880.93\n",
            "total_trades: 69996\n",
            "Sharpe: 0.994\n",
            "=================================\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 8         |\n",
            "|    fps             | 48        |\n",
            "|    time_elapsed    | 474       |\n",
            "|    total_timesteps | 23144     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 980       |\n",
            "|    critic_loss     | 109       |\n",
            "|    ent_coef        | 0.113     |\n",
            "|    ent_coef_loss   | -99.6     |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 23043     |\n",
            "|    reward          | 13.968004 |\n",
            "----------------------------------\n",
            "----------------------------------\n",
            "| time/              |           |\n",
            "|    episodes        | 12        |\n",
            "|    fps             | 48        |\n",
            "|    time_elapsed    | 713       |\n",
            "|    total_timesteps | 34716     |\n",
            "| train/             |           |\n",
            "|    actor_loss      | 473       |\n",
            "|    critic_loss     | 323       |\n",
            "|    ent_coef        | 0.0362    |\n",
            "|    ent_coef_loss   | -127      |\n",
            "|    learning_rate   | 0.0001    |\n",
            "|    n_updates       | 34615     |\n",
            "|    reward          | 4.1208844 |\n",
            "----------------------------------\n",
            "day: 2892, episode: 140\n",
            "begin_total_asset: 1000000.00\n",
            "end_total_asset: 4552851.79\n",
            "total_reward: 3552851.79\n",
            "total_cost: 5195.59\n",
            "total_trades: 51400\n",
            "Sharpe: 0.748\n",
            "=================================\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 952      |\n",
            "|    total_timesteps | 46288    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 249      |\n",
            "|    critic_loss     | 13.7     |\n",
            "|    ent_coef        | 0.0117   |\n",
            "|    ent_coef_loss   | -121     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 46187    |\n",
            "|    reward          | 7.655495 |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 1190     |\n",
            "|    total_timesteps | 57860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 136      |\n",
            "|    critic_loss     | 11.1     |\n",
            "|    ent_coef        | 0.00395  |\n",
            "|    ent_coef_loss   | -73.6    |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 57759    |\n",
            "|    reward          | 9.56089  |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 48       |\n",
            "|    time_elapsed    | 1434     |\n",
            "|    total_timesteps | 69432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | 69.6     |\n",
            "|    critic_loss     | 5.86     |\n",
            "|    ent_coef        | 0.00176  |\n",
            "|    ent_coef_loss   | 2.43     |\n",
            "|    learning_rate   | 0.0001   |\n",
            "|    n_updates       | 69331    |\n",
            "|    reward          | 9.461855 |\n",
            "---------------------------------\n"
          ]
        }
      ],
      "source": [
        "trained_sac = agent.train_model(model=model_sac,\n",
        "                             tb_log_name='sac',\n",
        "                             total_timesteps=70000) if if_using_sac else None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_SpZoQgPv7GO"
      },
      "outputs": [],
      "source": [
        "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
